{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./vitnyc.pickle', 'rb')as f:\n",
    "    vit = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./nycfeature.pickle', 'rb')as f:\n",
    "    nycfeature = pickle.load(f)\n",
    "\n",
    "nycfeature = nycfeature[0].numpy()\n",
    "\n",
    "with open('./nyclabel.pickle', 'rb')as f:\n",
    "    nyclabel = pickle.load(f)\n",
    "\n",
    "nyclabel = nyclabel[0].numpy()\n",
    "\n",
    "with open('./vitnyc.pickle', 'rb')as f:\n",
    "    vit = pickle.load(f)\n",
    "\n",
    "vit = vit[0].cpu().detach().numpy()\n",
    "\n",
    "with open('./swinvpnyc.pickle', 'rb')as f:\n",
    "    swinvp = pickle.load(f)\n",
    "\n",
    "swinvp = swinvp[0].cpu().detach().numpy()\n",
    "\n",
    "with open('./simvpnyc.pickle', 'rb')as f:\n",
    "    simvp = pickle.load(f)\n",
    "\n",
    "simvp = simvp[0].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax(data):\n",
    "    data_min = data.min()\n",
    "    data_max = data.max()\n",
    "\n",
    "    norm_data = (data-data_min) / (data_max - data_min)\n",
    "    norm_data = (norm_data * 255.0).astype(np.uint8)\n",
    "    return norm_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnycfeature = minmax(np.squeeze(nycfeature[0, 0, 0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.swapaxes(nycfeature[0, 0, 0:1], 0, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9089516"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nycfeature.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 7, 48, 20, 20)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nycfeature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1, 20, 20)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyclabel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0, 127,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0, 127,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0, 255,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0, 255,   0,   0,   0, 127,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0, 255, 127, 127,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0, 127,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minmax(np.squeeze(nyclabel[0,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 7 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 7, figsize=(7, 3))\n",
    "for i in range(7):\n",
    "    if i==0:\n",
    "        ax[i].set_title(\"Input\", fontsize=12, loc='left')\n",
    "    ax[i].imshow(Image.fromarray(minmax(np.squeeze(nycfeature[index, i, 0:1]))))\n",
    "    ax[i].axis('off')\n",
    "fig.savefig('Input.png', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3))\n",
    "ax.set_title(\"GroundTruth\", fontsize=12, loc='left')\n",
    "ax.imshow(Image.fromarray(minmax(np.squeeze(nyclabel[index,]))))\n",
    "ax.axis('off')\n",
    "fig.savefig('GT.png', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3))\n",
    "ax.set_title(\"C-ViT\", fontsize=12, loc='left')\n",
    "ax.imshow(Image.fromarray(minmax(np.squeeze(vit[index,]))))\n",
    "ax.axis('off')\n",
    "fig.savefig('vit.png', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3))\n",
    "ax.set_title(\"SimVP\", fontsize=12, loc='left')\n",
    "ax.imshow(Image.fromarray(minmax(np.squeeze(simvp[index,]))))\n",
    "ax.axis('off')\n",
    "fig.savefig('simvp.png', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(7, 3))\n",
    "ax.set_title(\"SwinVP\", fontsize=12, loc='left')\n",
    "ax.imshow(Image.fromarray(minmax(np.squeeze(swinvp[index,]))))\n",
    "ax.axis('off')\n",
    "fig.savefig('swinvp.png', transparent=True, bbox_inches='tight', pad_inches=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seq(data, seq_len, mode='mmnist'):\n",
    "    fig, ax = plt.subplots(2, seq_len, figsize=(seq_len, 3))\n",
    "    count = random.randint(0, data.shape[0]-seq_len)\n",
    "    fig.suptitle(f'Dataset_{count}th', fontweight='bold', fontsize=15)\n",
    "\n",
    "    for i in range(seq_len):\n",
    "        if mode == 'mmnist':\n",
    "            ax[0, i].imshow(Image.fromarray(data[i, count]), cmap='gray', aspect='auto')\n",
    "            # ax[0, i].imshow(Image.fromarray(data[i, count]), aspect='auto')\n",
    "        else:\n",
    "            # ax[0, i].imshow(Image.fromarray(data[count+i]), cmap='gray', aspect='auto')\n",
    "            ax[0, i].imshow(Image.fromarray(data[count+i]), aspect='auto')\n",
    "        ax[0, i].axis('off')\n",
    "    for i in range(seq_len):\n",
    "        if mode == 'mmnist':\n",
    "            ax[1, i].imshow(Image.fromarray(data[seq_len+i, count]), cmap='gray', aspect='auto')\n",
    "            # ax[1, i].imshow(Image.fromarray(data[seq_len+i, count]), aspect='auto')\n",
    "        else:\n",
    "            # ax[1, i].imshow(Image.fromarray(data[count+seq_len+i]), cmap='gray', aspect='auto')\n",
    "            ax[1, i].imshow(Image.fromarray(data[count+seq_len+i]), aspect='auto')\n",
    "        ax[1, i].axis('off')\n",
    "\n",
    "    ax[0, 0].set_title(\"Input sequence\", fontsize=12, loc='left')\n",
    "    ax[1, 0].set_title(\"Output sequence\", fontsize=12, loc='left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1, 20, 20)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simvp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_res = vit_res.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequential(data, seq_len=24, normalize=False):\n",
    "    total_len = seq_len * 2\n",
    "\n",
    "    seqeunces = [\n",
    "        data[i:(i + total_len)]\n",
    "        for i in range(len(data) - total_len + 1)\n",
    "    ]\n",
    "\n",
    "    X = np.array([seqeunce[:seq_len] for seqeunce in seqeunces], dtype='float32')\n",
    "    Y = np.array([seqeunce[seq_len:] for seqeunce in seqeunces], dtype='float32')\n",
    "\n",
    "    if normalize:\n",
    "        max_val = np.max(data)\n",
    "        X = X / max_val\n",
    "        Y = Y / max_val\n",
    "        return X, Y, max_val\n",
    "    else:\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/chicago/all_data.pkl', 'rb') as f:\n",
    "with open('../data/nyc/all_data.pkl', 'rb') as f:\n",
    "    file = pickle.load(f).astype(np.float32)\n",
    "\n",
    "X, y = make_sequential(file, 7)\n",
    "\n",
    "X[:,:,0] = (X[:,:,0] - np.mean(X[:,:,0])) / np.std(X[:,:,0])\n",
    "X[:,:,33:40] = (X[:,:,33:40] - np.mean(X[:,:,33:40])) / np.std(X[:,:,33:40])\n",
    "X[:,:,40] = (X[:,:,40] - np.mean(X[:,:,40])) / np.std(X[:,:,40])\n",
    "X[:,:,46] = (X[:,:,46] - np.mean(X[:,:,46])) / np.std(X[:,:,46])\n",
    "X[:,:,47] = (X[:,:,47] - np.mean(X[:,:,47])) / np.std(X[:,:,47])\n",
    "\n",
    "y = np.expand_dims(y[:, 0, 0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c-vit\n",
    "vit =ViT(image_size=20, patch_size=5, num_classes=400, channels=7,\n",
    "            dim=64, depth=6, heads=8, mlp_dim=128, data_type = 'nyc')\n",
    "# swinvp\n",
    "swinvp =SwinVP(in_shape=X.shape[1:], patch_size=5, num_classes=400, channels=7,\n",
    "                dim=64, depth=6, heads=8, mlp_dim=128, data_type = 'nyc')\n",
    "# simvp\n",
    "simvp =SwinVP(in_shape=X.shape[1:], patch_size=5, num_classes=400, channels=7,\n",
    "                dim=64, depth=6, heads=8, mlp_dim=128, mode='simvp', data_type = 'nyc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit.load_state_dict(torch.load('./logs/[vit] best_nyc_00012.pth'))\n",
    "swinvp.load_state_dict(torch.load('./logs/[swinvp] best_nyc_00022.pth'))\n",
    "simvp.load_state_dict(torch.load('./logs/[simvp] best_nyc_00020.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Data.DataLoader(\n",
    "    Data.TensorDataset(\n",
    "        torch.from_numpy(X),\n",
    "        torch.from_numpy(y)\n",
    "    ),\n",
    "    batch_size=32\n",
    ")\n",
    "XX, yy = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "# vit\n",
    "vit = vit.to(device)\n",
    "XX, yy = XX.to(device), yy.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of XX[:,:,0,:,:]: torch.Size([32, 7, 20, 20])\n",
      "Shape of XX[:,:,1:,:,:].flatten(start_dim=1): torch.Size([32, 131600])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of XX[:,:,0,:,:]:\", XX[:,:,0,:,:].shape)\n",
    "print(\"Shape of XX[:,:,1:,:,:].flatten(start_dim=1):\", XX[:,:,1:,:,:].flatten(start_dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x131600 and 329x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dnslab_wolf\\Desktop\\KICS2024\\ViT-traffic-accident-risk\\model\\vit.py:247\u001b[0m, in \u001b[0;36mViT.forward\u001b[1;34m(self, img, non_risk_features, mask)\u001b[0m\n\u001b[0;32m    245\u001b[0m B \u001b[38;5;241m=\u001b[39m non_risk_features\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    246\u001b[0m non_risk_features \u001b[38;5;241m=\u001b[39m non_risk_features\u001b[38;5;241m.\u001b[39mview(B, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 247\u001b[0m non_risk_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpast_region_features_fc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_risk_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_head((x \u001b[38;5;241m+\u001b[39m non_risk_features))\u001b[38;5;241m.\u001b[39mview(img\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x131600 and 329x64)"
     ]
    }
   ],
   "source": [
    "vit(XX[:,:,0,:,:], XX[:,:,1:,:,:].flatten(start_dim=1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit.load_state_dict(torch.load('./logs/[vit] best_nyc_00012.pth'))\n",
    "swinvp.load_state_dict(torch.load('./logs/[swinvp] best_nyc_00022.pth'))\n",
    "simvp.load_state_dict(torch.load('./logs/[simvp] best_nyc_00020.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = Data.DataLoader(\n",
    "    Data.TensorDataset(\n",
    "        torch.from_numpy(X),\n",
    "        torch.from_numpy(y)\n",
    "    ),\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX, yy = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX = XX.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinVP(\n",
       "  (header): GL3D(\n",
       "    (conv): Conv3d(48, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "    (sigconv): Sequential(\n",
       "      (0): Conv3d(48, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
       "      (1): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (enc): Sequential(\n",
       "    (0): StageModule(\n",
       "      (patch_partition): PatchMerging(\n",
       "        (patch_merge): Unfold(kernel_size=1, dilation=1, padding=0, stride=1)\n",
       "        (linear): Linear(in_features=48, out_features=16, bias=True)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x ModuleList(\n",
       "          (0): SwinBlock(\n",
       "            (attention_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): WindowAttention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=192, bias=False)\n",
       "                  (to_out): Linear(in_features=64, out_features=16, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (mlp_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "                    (1): GELU(approximate='none')\n",
       "                    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): SwinBlock(\n",
       "            (attention_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): WindowAttention(\n",
       "                  (cyclic_shift): CyclicShift()\n",
       "                  (cyclic_back_shift): CyclicShift()\n",
       "                  (to_qkv): Linear(in_features=16, out_features=192, bias=False)\n",
       "                  (to_out): Linear(in_features=64, out_features=16, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (mlp_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "                    (1): GELU(approximate='none')\n",
       "                    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (3): StageModule(\n",
       "      (patch_partition): PatchMerging(\n",
       "        (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
       "        (linear): Linear(in_features=64, out_features=16, bias=True)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x ModuleList(\n",
       "          (0): SwinBlock(\n",
       "            (attention_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): WindowAttention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=192, bias=False)\n",
       "                  (to_out): Linear(in_features=64, out_features=16, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (mlp_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "                    (1): GELU(approximate='none')\n",
       "                    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): SwinBlock(\n",
       "            (attention_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): WindowAttention(\n",
       "                  (cyclic_shift): CyclicShift()\n",
       "                  (cyclic_back_shift): CyclicShift()\n",
       "                  (to_qkv): Linear(in_features=16, out_features=192, bias=False)\n",
       "                  (to_out): Linear(in_features=64, out_features=16, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (mlp_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "                    (1): GELU(approximate='none')\n",
       "                    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (6): StageModule(\n",
       "      (patch_partition): PatchMerging(\n",
       "        (patch_merge): Unfold(kernel_size=2, dilation=1, padding=0, stride=2)\n",
       "        (linear): Linear(in_features=64, out_features=16, bias=True)\n",
       "      )\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x ModuleList(\n",
       "          (0): SwinBlock(\n",
       "            (attention_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): WindowAttention(\n",
       "                  (to_qkv): Linear(in_features=16, out_features=192, bias=False)\n",
       "                  (to_out): Linear(in_features=64, out_features=16, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (mlp_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "                    (1): GELU(approximate='none')\n",
       "                    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): SwinBlock(\n",
       "            (attention_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): WindowAttention(\n",
       "                  (cyclic_shift): CyclicShift()\n",
       "                  (cyclic_back_shift): CyclicShift()\n",
       "                  (to_qkv): Linear(in_features=16, out_features=192, bias=False)\n",
       "                  (to_out): Linear(in_features=64, out_features=16, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (mlp_block): Residual(\n",
       "              (fn): PreNorm(\n",
       "                (norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
       "                (fn): FeedForward(\n",
       "                  (net): Sequential(\n",
       "                    (0): Linear(in_features=16, out_features=64, bias=True)\n",
       "                    (1): GELU(approximate='none')\n",
       "                    (2): Linear(in_features=64, out_features=16, bias=True)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "  )\n",
       "  (dec): Decoder(\n",
       "    (dec): Sequential(\n",
       "      (0): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): PixelShuffle(upscale_factor=2)\n",
       "          )\n",
       "          (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (1): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (2): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Sequential(\n",
       "            (0): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (1): PixelShuffle(upscale_factor=2)\n",
       "          )\n",
       "          (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (3): ConvSC(\n",
       "        (conv): BasicConv2d(\n",
       "          (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (norm): GroupNorm(2, 16, eps=1e-05, affine=True)\n",
       "          (act): SiLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (readout): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       "  (hid): MidMetaNet(\n",
       "    (enc): Sequential(\n",
       "      (0): MetaBlock(\n",
       "        (block): GASubBlock(\n",
       "          (norm1): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (attn): SpatialAttention(\n",
       "            (proj_1): Conv2d(112, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): GELU(approximate='none')\n",
       "            (spatial_gating_unit): AttentionModule(\n",
       "              (conv0): Conv2d(112, 112, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=112)\n",
       "              (conv_spatial): Conv2d(112, 112, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=112)\n",
       "              (conv1): Conv2d(112, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (proj_2): Conv2d(112, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.010)\n",
       "          (norm2): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (mlp): MixMlp(\n",
       "            (fc1): Conv2d(112, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(896, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (reduction): Conv2d(112, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): MetaBlock(\n",
       "        (block): GASubBlock(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (attn): SpatialAttention(\n",
       "            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): GELU(approximate='none')\n",
       "            (spatial_gating_unit): AttentionModule(\n",
       "              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)\n",
       "              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.007)\n",
       "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (mlp): MixMlp(\n",
       "            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): MetaBlock(\n",
       "        (block): GASubBlock(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (attn): SpatialAttention(\n",
       "            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): GELU(approximate='none')\n",
       "            (spatial_gating_unit): AttentionModule(\n",
       "              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)\n",
       "              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (drop_path): DropPath(drop_prob=0.003)\n",
       "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (mlp): MixMlp(\n",
       "            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): MetaBlock(\n",
       "        (block): GASubBlock(\n",
       "          (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (attn): SpatialAttention(\n",
       "            (proj_1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): GELU(approximate='none')\n",
       "            (spatial_gating_unit): AttentionModule(\n",
       "              (conv0): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=256)\n",
       "              (conv_spatial): Conv2d(256, 256, kernel_size=(7, 7), stride=(1, 1), padding=(9, 9), dilation=(3, 3), groups=256)\n",
       "              (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            )\n",
       "            (proj_2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (mlp): MixMlp(\n",
       "            (fc1): Conv2d(256, 2048, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "            )\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (reduction): Conv2d(256, 112, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "swinvp.to(device)\n",
    "# loader.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = swinvp(XX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 20, 20])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.38 GiB. GPU 0 has a total capacty of 8.00 GiB of which 2.55 GiB is free. Of the allocated memory 4.40 GiB is allocated by PyTorch, and 10.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dnslab_wolf\\Desktop\\KICS2024\\ViT-traffic-accident-risk\\model\\SwinVP.py:322\u001b[0m, in \u001b[0;36mSwinVP.forward\u001b[1;34m(self, x_raw)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    320\u001b[0m     x \u001b[38;5;241m=\u001b[39m x_raw\u001b[38;5;241m.\u001b[39mreshape(B \u001b[38;5;241m*\u001b[39m T, C, H, W)\n\u001b[1;32m--> 322\u001b[0m skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    323\u001b[0m embed \u001b[38;5;241m=\u001b[39m skip\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc)):\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dnslab_wolf\\Desktop\\KICS2024\\ViT-traffic-accident-risk\\model\\SwinTransformer.py:199\u001b[0m, in \u001b[0;36mStageModule.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    196\u001b[0m \n\u001b[0;32m    197\u001b[0m     \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_partition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m regular_block, shifted_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m    201\u001b[0m         x \u001b[38;5;241m=\u001b[39m regular_block(x)\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dnslab_wolf\\Desktop\\KICS2024\\ViT-traffic-accident-risk\\model\\SwinTransformer.py:168\u001b[0m, in \u001b[0;36mPatchMerging.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    164\u001b[0m new_h, new_w \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownscaling_factor, w \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownscaling_factor\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# print(new_h, new_w)\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_merge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(b, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, new_h, new_w)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# print(self.linear)\u001b[39;00m\n\u001b[0;32m    173\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(x)\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\fold.py:297\u001b[0m, in \u001b[0;36mUnfold.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munfold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\functional.py:4756\u001b[0m, in \u001b[0;36munfold\u001b[1;34m(input, kernel_size, dilation, padding, stride)\u001b[0m\n\u001b[0;32m   4752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m   4753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   4754\u001b[0m         unfold, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, kernel_size, dilation\u001b[38;5;241m=\u001b[39mdilation, padding\u001b[38;5;241m=\u001b[39mpadding, stride\u001b[38;5;241m=\u001b[39mstride\n\u001b[0;32m   4755\u001b[0m     )\n\u001b[1;32m-> 4756\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim2col\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.38 GiB. GPU 0 has a total capacty of 8.00 GiB of which 2.55 GiB is free. Of the allocated memory 4.40 GiB is allocated by PyTorch, and 10.22 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "model(torch.Tensor(X).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../data/chicago/all_data.pkl', 'rb') as f:\n",
    "with open('../data/nyc/all_data.pkl', 'rb') as f:\n",
    "    file = pickle.load(f).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 48, 20, 20)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = file[:,39,]\n",
    "output = file[:,40,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, _ = make_sequential(file, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.expand_dims(X, 2)\n",
    "# y = np.expand_dims(y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8747, 7, 48, 20, 20), (8747, 1, 20, 20))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5819, 7, 1, 20, 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model = SwinVP(X.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # c-vit\n",
    "# model =ViT(image_size=20, patch_size=5, num_classes=400, channels=7,\n",
    "#             dim=64, depth=6, heads=8, mlp_dim=128, data_type = 'chicago')\n",
    "# swinvp\n",
    "model =SwinVP(in_shape=X.shape[1:], patch_size=5, num_classes=400, channels=7,\n",
    "                dim=64, depth=6, heads=8, mlp_dim=128, data_type = 'chicago')\n",
    "# # simvp\n",
    "# model =SwinVP(in_shape=X.shape[1:], patch_size=5, num_classes=400, channels=7,\n",
    "#                 dim=64, depth=6, heads=8, mlp_dim=128, mode='simvp', data_type = 'chicago')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for SwinVP:\n\tMissing key(s) in state_dict: \"header.conv.weight\", \"header.conv.bias\", \"header.sigconv.0.weight\", \"header.sigconv.0.bias\". \n\tsize mismatch for enc.0.patch_partition.linear.weight: copying a param with shape torch.Size([16, 41]) from checkpoint, the shape in current model is torch.Size([16, 1]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./logs/best_chicago_00060.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\jhs_project\\jhs_torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for SwinVP:\n\tMissing key(s) in state_dict: \"header.conv.weight\", \"header.conv.bias\", \"header.sigconv.0.weight\", \"header.sigconv.0.bias\". \n\tsize mismatch for enc.0.patch_partition.linear.weight: copying a param with shape torch.Size([16, 41]) from checkpoint, the shape in current model is torch.Size([16, 1])."
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./logs/[swinvp] best_nyc_00060.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 7, 1, 20, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 20, 20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor(X[0:32])).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jhs_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
